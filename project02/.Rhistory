se = mean(se),
ds = mean(ds)
)
knitr::kable(result_summary)
ggplot() +
geom_point(
result.leave1out,
mapping = aes(x = se, y = ds, color = Model),
size = 2
) +
geom_point(
result_summary,
mapping = aes(x = se, y = ds, color = Model),
shape = 4,
size = 5,
) +
labs(title = "Scores and Average Scores of Model A and B", x = "Squared Error", y = "DS Score")
# S_Delta for each prediction
score_diff <- data.frame(
se = result_A.leave1out$se - result_B.leave1out$se,
ds = result_A.leave1out$ds - result_B.leave1out$ds
)
# test statistic
statistic0 <- score_diff %>%
summarise(
se = mean(se),
ds = mean(ds)
)
# sample number
J <- 10000
# sampling
statistic <- data.frame(
se = numeric(J),
ds = numeric(J)
)
for (loop in seq_len(J)) {
random_sign <- sample(
x = c(-1, 1),
size = nrow(score_diff),
replace = TRUE
)
statistic[loop, ] <- score_diff %>% summarise(
se = mean(random_sign * se),
ds = mean(random_sign * ds)
)
}
# p-value computation
p_values <-
statistic %>%
summarise(
se = mean(se > statistic0$se),
ds = mean(ds > statistic0$ds)
)
rownames(p_values) <- "p-value"
knitr::kable(p_values)
statistic_long <- pivot_longer(statistic, cols = c(se, ds), names_to = "Score", values_to = "Value")
statistic0_long <- pivot_longer(statistic0, cols = c(se, ds), names_to = "Score", values_to = "Value")
ggplot() +
geom_jitter(
data = statistic_long,
aes(x = Score, y = Value, color = "Monte Carlo"),
width = 0.2, alpha = 0.7
) +
labs(
title = "Monte Carlo Sampling of SE and DS Score Statistic",
x = "Scores", y = "Statistics",
color = "ds"
) +
geom_point(
data = statistic0_long,
aes(x = Score, y = Value, color = "test statistic")
) +
scale_color_manual(
values = c("Monte Carlo" = "skyblue", "test statistic" = "red"),
name = "Statistics"
)
standard.error <- data.frame(
se = sd(statistic$se) / sqrt(J),
ds = sd(statistic$ds) / sqrt(J)
)
rownames(standard.error) <- "Standard Error"
knitr::kable(standard.error)
# sample number
k <- 100
# sampling
N.sample <- rgeom(n = k, prob = 1 / (1001))
Phi.sample <- rbeta(n = k, shape1 = 2, shape2 = 2)
params <- data.frame(
N = N.sample,
Phi = Phi.sample
)
# loglike
loglike <- arch_loglike(
y = c(256, 237),
params = params
)
knitr::kable(head(data.frame(
N = N.sample,
Phi = Phi.sample,
loglike = loglike
)))
MC.estimation <- estimate(
y = c(256, 237),
xi = 1 / 1001,
a = 0.5, b = 0.5,
K = 10000
)
knitr::kable(MC.estimation)
# Modify this setup code chunk to set options
# or add extra packages etc if needed.
# See the project instructions for more details
# on what code to show, and where/how.
# Set default code chunk options
knitr::opts_chunk$set(
echo = TRUE,
eval = TRUE
)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(ggplot2))
theme_set(theme_bw())
# To give the same random number sequence every time the document is knit:ed,
# making it easier to discuss the specific numbers in the text:
set.seed(12345L)
# Do not change this code chunk
# Load function definitions
source("code.R")
load("filament1.rda")
pred_A <- filament1_predict(
data = filament1,
model = "A",
newdata = filament1
)
knitr::kable(head(pred_A))
pred_B <- filament1_predict(
data = filament1,
model = "B",
newdata = filament1
)
knitr::kable(head(pred_B))
ggplot(
rbind(
cbind(pred_A, filament1, Model = "A"),
cbind(pred_B, filament1, Model = "B")
),
mapping = aes(CAD_Weight)
) +
geom_line(aes(y = mean, col = Model)) +
geom_ribbon(aes(ymin = lwr, ymax = upr, fill = Model), alpha = 0.25) +
geom_point(aes(y = Actual_Weight), data = filament1) +
labs(title = "Prediction Distribution", y = "Actual_Weight")
score_A <- filament1_score(
data = filament1,
model = "A",
newdata = filament1
)
knitr::kable(head(score_A))
score_B <- filament1_score(
data = filament1,
model = "B",
newdata = filament1
)
knitr::kable(head(score_B))
ggplot(rbind(
cbind(score_A, Model = "A"),
cbind(score_B, Model = "B")
), aes(x = se, y = ds, color = Model)) +
geom_point(size = 2) +
labs(title = "Scores of Model A and B", x = "Squared Error", y = "DS Score")
result_A.leave1out <- leave1out(
data = filament1,
model = "A"
)
knitr::kable(head(result_A.leave1out))
result_B.leave1out <- leave1out(
data = filament1,
model = "B"
)
knitr::kable(head(result_B.leave1out))
result.leave1out <- rbind(
cbind(result_A.leave1out, Model = "A"),
cbind(result_B.leave1out, Model = "B")
)
ggplot(result.leave1out,
mapping = aes(CAD_Weight)
) +
geom_line(aes(y = mean, col = Model)) +
geom_point(aes(y = Actual_Weight), data = filament1) +
labs(title = "leave-one-out Validation", y = "Actual_Weight")
result.leave1out_long <- pivot_longer(result.leave1out, cols = c(se, ds), names_to = "Score", values_to = "Value")
ggplot(data = result.leave1out_long) +
geom_point(aes(x = CAD_Weight, y = Value, color = Model)) +
facet_wrap(~Score) +
labs(title = "Score versus the CAD_weight for Model A and B", y = "Score")
result_summary <- result.leave1out %>%
group_by(Model) %>%
summarise(
se = mean(se),
ds = mean(ds)
)
knitr::kable(result_summary)
ggplot() +
geom_point(
result.leave1out,
mapping = aes(x = se, y = ds, color = Model),
size = 2
) +
geom_point(
result_summary,
mapping = aes(x = se, y = ds, color = Model),
shape = 4,
size = 5,
) +
labs(title = "Scores and Average Scores of Model A and B", x = "Squared Error", y = "DS Score")
# S_Delta for each prediction
score_diff <- data.frame(
se = result_A.leave1out$se - result_B.leave1out$se,
ds = result_A.leave1out$ds - result_B.leave1out$ds
)
# test statistic
statistic0 <- score_diff %>%
summarise(
se = mean(se),
ds = mean(ds)
)
# sample number
J <- 10000
# sampling
statistic <- data.frame(
se = numeric(J),
ds = numeric(J)
)
for (loop in seq_len(J)) {
random_sign <- sample(
x = c(-1, 1),
size = nrow(score_diff),
replace = TRUE
)
statistic[loop, ] <- score_diff %>% summarise(
se = mean(random_sign * se),
ds = mean(random_sign * ds)
)
}
# p-value computation
p_values <-
statistic %>%
summarise(
se = mean(se > statistic0$se),
ds = mean(ds > statistic0$ds)
)
rownames(p_values) <- "p-value"
knitr::kable(p_values)
statistic_long <- pivot_longer(statistic, cols = c(se, ds), names_to = "Score", values_to = "Value")
statistic0_long <- pivot_longer(statistic0, cols = c(se, ds), names_to = "Score", values_to = "Value")
ggplot() +
geom_jitter(
data = statistic_long,
aes(x = Score, y = Value, color = "Monte Carlo"),
width = 0.2, alpha = 0.7
) +
labs(
title = "Monte Carlo Sampling of SE and DS Score Statistic",
x = "Scores", y = "Statistics",
color = "ds"
) +
geom_point(
data = statistic0_long,
aes(x = Score, y = Value, color = "test statistic")
) +
scale_color_manual(
values = c("Monte Carlo" = "skyblue", "test statistic" = "red"),
name = "Statistics"
)
standard.error.T <- data.frame(
se = sd(statistic$se) / sqrt(J),
ds = sd(statistic$ds) / sqrt(J)
)
rownames(standard.error.T) <- "Standard Error"
knitr::kable(standard.error.T)
standard.error.P <- data.frame(
se = sd(statistic$se > statistic0$se) / sqrt(J),
ds = sd(statistic$ds > statistic0$ds) / sqrt(J)
)
rownames(standard.error.P) <- "Standard Error"
knitr::kable(standard.error.P)
# sample number
k <- 100
# sampling
N.sample <- rgeom(n = k, prob = 1 / (1001))
Phi.sample <- rbeta(n = k, shape1 = 2, shape2 = 2)
params <- data.frame(
N = N.sample,
Phi = Phi.sample
)
# loglike
loglike <- arch_loglike(
y = c(256, 237),
params = params
)
knitr::kable(head(data.frame(
N = N.sample,
Phi = Phi.sample,
loglike = loglike
)))
MC.estimation <- estimate(
y = c(256, 237),
xi = 1 / 1001,
a = 0.5, b = 0.5,
K = 10000
)
knitr::kable(MC.estimation)
# Modify this setup code chunk to set options
# or add extra packages etc if needed.
# See the project instructions for more details
# on what code to show, and where/how.
# Set default code chunk options
knitr::opts_chunk$set(
echo = TRUE,
eval = TRUE
)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(ggplot2))
theme_set(theme_bw())
# To give the same random number sequence every time the document is knit:ed,
# making it easier to discuss the specific numbers in the text:
set.seed(12345L)
# Do not change this code chunk
# Load function definitions
source("code.R")
load("filament1.rda")
pred_A <- filament1_predict(
data = filament1,
model = "A",
newdata = filament1
)
knitr::kable(head(pred_A))
pred_B <- filament1_predict(
data = filament1,
model = "B",
newdata = filament1
)
knitr::kable(head(pred_B))
ggplot(
rbind(
cbind(pred_A, filament1, Model = "A"),
cbind(pred_B, filament1, Model = "B")
),
mapping = aes(CAD_Weight)
) +
geom_line(aes(y = mean, col = Model)) +
geom_ribbon(aes(ymin = lwr, ymax = upr, fill = Model), alpha = 0.25) +
geom_point(aes(y = Actual_Weight), data = filament1) +
labs(title = "Prediction Distribution", y = "Actual_Weight")
score_A <- filament1_score(
data = filament1,
model = "A",
newdata = filament1
)
knitr::kable(head(score_A))
score_B <- filament1_score(
data = filament1,
model = "B",
newdata = filament1
)
knitr::kable(head(score_B))
ggplot(rbind(
cbind(score_A, Model = "A"),
cbind(score_B, Model = "B")
), aes(x = se, y = ds, color = Model)) +
geom_point(size = 2) +
labs(title = "Scores of Model A and B", x = "Squared Error", y = "DS Score")
result_A.leave1out <- leave1out(
data = filament1,
model = "A"
)
knitr::kable(head(result_A.leave1out))
result_B.leave1out <- leave1out(
data = filament1,
model = "B"
)
knitr::kable(head(result_B.leave1out))
result.leave1out <- rbind(
cbind(result_A.leave1out, Model = "A"),
cbind(result_B.leave1out, Model = "B")
)
ggplot(result.leave1out,
mapping = aes(CAD_Weight)
) +
geom_line(aes(y = mean, col = Model)) +
geom_point(aes(y = Actual_Weight), data = filament1) +
labs(title = "leave-one-out Validation", y = "Actual_Weight")
result.leave1out_long <- pivot_longer(result.leave1out, cols = c(se, ds), names_to = "Score", values_to = "Value")
ggplot(data = result.leave1out_long) +
geom_point(aes(x = CAD_Weight, y = Value, color = Model)) +
facet_wrap(~Score) +
labs(title = "Score versus the CAD_weight for Model A and B", y = "Score")
result_summary <- result.leave1out %>%
group_by(Model) %>%
summarise(
se = mean(se),
ds = mean(ds)
)
knitr::kable(result_summary)
ggplot() +
geom_point(
result.leave1out,
mapping = aes(x = se, y = ds, color = Model),
size = 2
) +
geom_point(
result_summary,
mapping = aes(x = se, y = ds, color = Model),
shape = 4,
size = 5,
) +
labs(title = "Scores and Average Scores of Model A and B", x = "Squared Error", y = "DS Score")
# S_Delta for each prediction
score_diff <- data.frame(
se = result_A.leave1out$se - result_B.leave1out$se,
ds = result_A.leave1out$ds - result_B.leave1out$ds
)
# test statistic
statistic0 <- score_diff %>%
summarise(
se = mean(se),
ds = mean(ds)
)
# sample number
J <- 10000
# sampling
statistic <- data.frame(
se = numeric(J),
ds = numeric(J)
)
for (loop in seq_len(J)) {
random_sign <- sample(
x = c(-1, 1),
size = nrow(score_diff),
replace = TRUE
)
statistic[loop, ] <- score_diff %>% summarise(
se = mean(random_sign * se),
ds = mean(random_sign * ds)
)
}
# p-value computation
p_values <-
statistic %>%
summarise(
se = mean(se > statistic0$se),
ds = mean(ds > statistic0$ds)
)
rownames(p_values) <- "p-value"
knitr::kable(p_values)
statistic_long <- pivot_longer(statistic, cols = c(se, ds), names_to = "Score", values_to = "Value")
statistic0_long <- pivot_longer(statistic0, cols = c(se, ds), names_to = "Score", values_to = "Value")
ggplot() +
geom_jitter(
data = statistic_long,
aes(x = Score, y = Value, color = "Monte Carlo"),
width = 0.2, alpha = 0.7
) +
labs(
title = "Monte Carlo Sampling of SE and DS Score Statistic",
x = "Scores", y = "Statistics",
color = "ds"
) +
geom_point(
data = statistic0_long,
aes(x = Score, y = Value, color = "test statistic")
) +
scale_color_manual(
values = c("Monte Carlo" = "skyblue", "test statistic" = "red"),
name = "Statistics"
)
standard.error.T <- data.frame(
se = sd(statistic$se) / sqrt(J),
ds = sd(statistic$ds) / sqrt(J)
)
rownames(standard.error.T) <- "Standard Error"
knitr::kable(standard.error.T)
standard.error.P <- data.frame(
se = sd(statistic$se > statistic0$se) / sqrt(J),
ds = sd(statistic$ds > statistic0$ds) / sqrt(J)
)
rownames(standard.error.P) <- "Standard Error"
knitr::kable(standard.error.P)
# sample number
k <- 100
# sampling
N.sample <- rgeom(n = k, prob = 1 / (1001))
Phi.sample <- rbeta(n = k, shape1 = 2, shape2 = 2)
params <- data.frame(
N = N.sample,
Phi = Phi.sample
)
# loglike
loglike <- arch_loglike(
y = c(256, 237),
params = params
)
knitr::kable(head(data.frame(
N = N.sample,
Phi = Phi.sample,
loglike = loglike
)))
MC.estimation <- estimate(
y = c(256, 237),
xi = 1 / 1001,
a = 0.5, b = 0.5,
K = 10000
)
knitr::kable(MC.estimation)
